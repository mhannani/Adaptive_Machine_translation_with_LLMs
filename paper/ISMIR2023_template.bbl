% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann \emph{et~al.}, ``Palm: Scaling language
  modeling with pathways,'' \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{penedo2023refinedweb}
P.~et~al., ``The refinedweb dataset for falcon llm: outperforming curated
  corpora with web data, and web data only,'' \emph{arXiv preprint
  arXiv:2306.01116}, 2023.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar \emph{et~al.}, ``Llama: Open
  and efficient foundation language models,'' \emph{arXiv preprint
  arXiv:2302.13971}, 2023.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray \emph{et~al.}, ``Training language models to
  follow instructions with human feedback,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~35, pp. 27\,730--27\,744, 2022.

\bibitem{lison2016opensubtitles2016}
P.~Lison and J.~Tiedemann, ``Opensubtitles2016: Extracting large parallel
  corpora from movie and tv subtitles,'' 2016.

\bibitem{knowles2018fuzzy}
R.~Knowles and P.~Koehn, ``Fuzzy match incorporation for neural machine
  translation,'' \emph{arXiv preprint arXiv:1806.08117}, 2018.

\bibitem{bulte2019fuzzy}
B.~Bulte and A.~A. Tezcan, ``Fuzzy matches for improving the consistency of
  neural machine translation,'' \emph{arXiv preprint arXiv:1903.11534}, 2019.

\bibitem{xu2020forcing}
e.~a. Xu, Yukun, ``Forcing the translation of new sentence pairs to adapt to
  fuzzy matches in context,'' \emph{arXiv preprint arXiv:2005.02974}, 2020.

\bibitem{hosseini2020embedding}
S.~M.~S. Hosseini and H.~Zamani, ``Embedding similarity-based fuzzy match
  retrieval for neural machine translation,'' \emph{arXiv preprint
  arXiv:2010.04006}, 2020.

\bibitem{he2022incorporating}
e.~a. He, Jing, ``Incorporating fuzzy matches into neural machine translation
  using self-attention,'' \emph{arXiv preprint arXiv:2207.02902}, 2022.

\bibitem{wang2021more}
e.~a. Wang, Yukun, ``A more efficient and accurate embedding similarity-based
  fuzzy match retrieval algorithm for neural machine translation,'' \emph{arXiv
  preprint arXiv:2106.03760}, 2021.

\bibitem{pham2021fuzzy}
e.~a. Pham, Minh, ``Fuzzy matches for improving low-resource language
  translation,'' \emph{arXiv preprint arXiv:2101.00501}, 2021.

\bibitem{li2023fuzzy}
e.~a. Li, Haoran, ``Fuzzy matches for improving the translation of legal
  documents,'' \emph{arXiv preprint arXiv:2302.13211}, 2023.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\end{thebibliography}
