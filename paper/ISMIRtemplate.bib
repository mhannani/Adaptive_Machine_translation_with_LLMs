% Example bibtex file for ISMIR Template

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo et al.},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{lison2016opensubtitles2016,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{knowles2018fuzzy,
title={Fuzzy Match Incorporation for Neural Machine Translation},
author={Knowles, Rebecca, and Philipp Koehn},
journal={arXiv preprint arXiv:1806.08117},
year={2018}
}

@article{bulte2019fuzzy,
title={Fuzzy matches for improving the consistency of neural machine translation},
author={Bulte, Boris, and Ayşe Aişe Tezcan},
journal={arXiv preprint arXiv:1903.11534},
year={2019}
}

@article{xu2020forcing,
title={Forcing the translation of new sentence pairs to adapt to fuzzy matches in context},
author={Xu, Yukun, et al.},
journal={arXiv preprint arXiv:2005.02974},
year={2020}
}

@article{hosseini2020embedding,
title={Embedding similarity-based fuzzy match retrieval for neural machine translation},
author={Hosseini, Seyed Mohammad Sadegh, and Hamed Zamani},
journal={arXiv preprint arXiv:2010.04006},
year={2020}
}

@article{pham2021fuzzy,
title={Fuzzy matches for improving low-resource language translation},
author={Pham, Minh, et al.},
journal={arXiv preprint arXiv:2101.00501},
year={2021}
}

@article{he2022incorporating,
title={Incorporating fuzzy matches into neural machine translation using self-attention},
author={He, Jing, et al.},
journal={arXiv preprint arXiv:2207.02902},
year={2022}
}

@article{wang2021more,
title={A more efficient and accurate embedding similarity-based fuzzy match retrieval algorithm for neural machine translation},
author={Wang, Yukun, et al.},
journal={arXiv preprint arXiv:2106.03760},
year={2021}
}

@article{li2023fuzzy,
title={Fuzzy matches for improving the translation of legal documents},
author={Li, Haoran, et al.},
journal={arXiv preprint arXiv:2302.13211},
year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@online{langchain,
  author = {LangChain},
  title = {Introduction — langchain},
  year = {2023},
  url = {https://python.langchain.com/en/latest/index.html},
}

@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@inproceedings{babenko2016efficient,
  title={Efficient indexing of billion-scale datasets of deep descriptors},
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2055--2063},
  year={2016}
}

@article{vilar2022prompting,
  title={Prompting palm for translation: Assessing strategies and performance},
  author={Vilar, David and Freitag, Markus and Cherry, Colin and Luo, Jiaming and Ratnakar, Viresh and Foster, George},
  journal={arXiv preprint arXiv:2211.09102},
  year={2022}
}

@article{wang2021language,
  title={Language models are good translators},
  author={Wang, Shuo and Tu, Zhaopeng and Tan, Zhixing and Wang, Wenxuan and Sun, Maosong and Liu, Yang},
  journal={arXiv preprint arXiv:2106.13627},
  year={2021}
}

@article{moslem2022domain,
  title={Domain-specific text generation for machine translation},
  author={Moslem, Yasmin and Haque, Rejwanul and Kelleher, John D and Way, Andy},
  journal={arXiv preprint arXiv:2208.05909},
  year={2022}
}

@article{agrawal2022context,
  title={In-context examples selection for machine translation},
  author={Agrawal, Sweta and Zhou, Chunting and Lewis, Mike and Zettlemoyer, Luke and Ghazvininejad, Marjan},
  journal={arXiv preprint arXiv:2212.02437},
  year={2022}
}

@article{zhang2023prompting,
  title={Prompting large language model for machine translation: A case study},
  author={Zhang, Biao and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:2301.07069},
  year={2023}
}

@inproceedings{bulte2019neural,
  title={Neural fuzzy repair: Integrating fuzzy matches into neural machine translation},
  author={Bulte, Bram and Tezcan, Arda},
  booktitle={57th Annual Meeting of the Association-for-Computational-Linguistics (ACL)},
  pages={1800--1809},
  year={2019}
}

@inproceedings{knowles2018comparison,
  title={A comparison of machine translation paradigms for use in black-box fuzzy-match repair},
  author={Knowles, Rebecca and Ortega, John and Koehn, Philipp},
  booktitle={Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing},
  pages={249--255},
  year={2018}
}

@inproceedings{xu2020boosting,
  title={Boosting neural machine translation with similar translations},
  author={Xu, Jitao and Crego, Josep-Maria and Senellart, Jean},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  pages={1570--1579},
  year={2020},
  organization={Association for Computational Linguistics}
}

@inproceedings{hosseini2020deezymatch,
  title={DeezyMatch: A flexible deep learning approach to fuzzy string matching},
  author={Hosseini, Kasra and Nanni, Federico and Ardanuy, Mariona Coll},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations},
  pages={62--69},
  year={2020}
}