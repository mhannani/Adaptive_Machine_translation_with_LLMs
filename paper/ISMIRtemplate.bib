% Example bibtex file for ISMIR Template

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo et al.},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{lison2016opensubtitles2016,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{knowles2018fuzzy,
title={Fuzzy Match Incorporation for Neural Machine Translation},
author={Knowles, Rebecca, and Philipp Koehn},
journal={arXiv preprint arXiv:1806.08117},
year={2018}
}

@article{bulte2019fuzzy,
title={Fuzzy matches for improving the consistency of neural machine translation},
author={Bulte, Boris, and Ayşe Aişe Tezcan},
journal={arXiv preprint arXiv:1903.11534},
year={2019}
}

@article{xu2020forcing,
title={Forcing the translation of new sentence pairs to adapt to fuzzy matches in context},
author={Xu, Yukun, et al.},
journal={arXiv preprint arXiv:2005.02974},
year={2020}
}

@article{hosseini2020embedding,
title={Embedding similarity-based fuzzy match retrieval for neural machine translation},
author={Hosseini, Seyed Mohammad Sadegh, and Hamed Zamani},
journal={arXiv preprint arXiv:2010.04006},
year={2020}
}

@article{pham2021fuzzy,
title={Fuzzy matches for improving low-resource language translation},
author={Pham, Minh, et al.},
journal={arXiv preprint arXiv:2101.00501},
year={2021}
}

@article{he2022incorporating,
title={Incorporating fuzzy matches into neural machine translation using self-attention},
author={He, Jing, et al.},
journal={arXiv preprint arXiv:2207.02902},
year={2022}
}

@article{wang2021more,
title={A more efficient and accurate embedding similarity-based fuzzy match retrieval algorithm for neural machine translation},
author={Wang, Yukun, et al.},
journal={arXiv preprint arXiv:2106.03760},
year={2021}
}

@article{li2023fuzzy,
title={Fuzzy matches for improving the translation of legal documents},
author={Li, Haoran, et al.},
journal={arXiv preprint arXiv:2302.13211},
year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}